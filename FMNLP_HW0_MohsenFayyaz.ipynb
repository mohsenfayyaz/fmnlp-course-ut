{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgNZTjrhcHa0"
      },
      "source": [
        "## Homework 0, FMNLP Fall 2022\n",
        "### You have ten days for this assignment, please submit as PDF(File>Print>Save as PDF). 100 points total.\n",
        "\n",
        "#### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n",
        "\n",
        "\n",
        "---\n",
        "If you have any further questions or concerns, contact the TA via email:\n",
        "sheydaes@gmail.com\n",
        "Telegram: @Sheyddy\n",
        "\n",
        "***LINK: https://colab.research.google.com/drive/137UvgZDql0WOmF8E6k9VhGAKXtcc1f5-?usp=sharing ***\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says \"Write your answer here...\" with your actual answer.\n",
        " \n",
        "- This assignment is designed so that you can run all cells almost instantly. If it is taking longer than that, you have made a mistake in your code.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to submit this problem set:*\n",
        "- Write all the answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to Elearn.\n",
        "  \n",
        "- **Important:** check your PDF before you submit to Elearn to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n",
        "\n",
        "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxa48WP8dXh9"
      },
      "source": [
        "##Question 1.1 (10 points)\n",
        "Let's begin with a quick probability review. In the task of language modeling, we're interested in computing the **joint** probability of some text. Say we have a sentence $s$ with $n$ words ($w_1, w_2, w_3, \\dots, w_n$) and we want to compute the joint probability $P(w_1, w_2, w_3, \\dots, w_n$). Assume we are given a model that produces the conditional probability of the next word in a sentence given all preceding words: $P(w_i|w_1,w_2,\\dots,w_{i-1})$. How can we use this model to compute the joint probability of sentence $s$?\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyrvDudjbPUO"
      },
      "source": [
        "**Write your answer here!** Please include an equation(s) formatted in LaTeX in your answer. You can add LaTeX to your answer by wrapping it in $ signs; see the above cell for examples.\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "Based on [chain rule](https://en.wikipedia.org/wiki/Chain_rule_(probability)), we can write the joint probability of all the words of the sentence as follows:\n",
        "$$\n",
        "\\begin{align*}\n",
        "P(w_1, w_2, w_3, \\dots, w_n)\n",
        "&=P(w_1)⋅P(w_2|w_1)⋅P(w_3|w_1w_2) \\dots P(w_n|w_1w_2 \\dots w_{n-1}) \\\\\n",
        "&=\\prod_{i=1}^{n} P(w_i|w_1w_2 \\dots w_{i-1})\n",
        "\\end{align*}\n",
        "$$\n",
        "As mentioned before we have $P(w_i|w_1,w_2,\\dots,w_{i-1})$; therefore, we can compute $P(w_1, w_2, w_3, \\dots, w_n)$ by calculating the production above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsq0NIDzcfeC"
      },
      "source": [
        "##Question 1.2 (10 points)\n",
        "Why would we ever want to compute the joint probability of a sentence? Provide **two** different reasons why this probability might be useful to solve an NLP task.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLR5ML-dQ2t"
      },
      "source": [
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "<blockquote>\n",
        "\n",
        "\n",
        "First, this is the definition of a language model and we can generate sentences by predicting the next word after a sequence of words (Max probability as the simplest method). This can help in downstream tasks such as summarization, translation and more. Second, it can also validate if a sentence is grammatically (or even factually) correct or not (Like Cola task in GLUE).\n",
        "Third, by pre-training such model (like GPT, unidirectional RNNs) we will have rich representations of the sentence which can help us in solving any NLP task, like sentiment analysis or topic classification. \n",
        "Moreover, prompting can be used in this case such as asking model after a sentene that \"the sentence is ?(good/bad)\" and see if the probability of good is higher or bad to determine the sentiment of the sentence. Undoubtedly, these were only a few examples of this joint probability applciations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM2aVbUcYPXv"
      },
      "source": [
        "##Question 1.3 (5 points)\n",
        "Here is a simple way to build a language model: for any prefix $w_1, w_2, \\dots, w_{i-1}$, retrieve all occurrences of that prefix in some huge text corpus (such as the [Common Crawl](https://https://commoncrawl.org/)) and keep count of the word $w_i$ that follows each occurrence. I can then use this to estimate the conditional probability $P(w_i|w_1, w_2, \\dots, w_{i-1})$ for any prefix. Explain why this method is completely impractical!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crODlcAKfGcu"
      },
      "source": [
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "First big issue of this idea is that we cannot expect to find the exact occurence in our corpus, even if it is very large. As the language is generative, anybody can make a new sentence, especially in literature and using novel metaphors can cause that. Therefore, we are likely to end up with lots of zeros when finding occurences of $w_1, w_2, \\dots, w_{i-1}$.\n",
        "Moreover, it is a very demanding algorithm because we go through a huge text corpus for each sentence (It can be made faster at the expense of storage, but still computationally impractical)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1_cCBOwfPxI"
      },
      "source": [
        "##Question 2.1 (5 points)\n",
        "Let's switch over to coding! The below coding cell contains the opening paragraph of Daphne du Maurier's novel *Rebecca*. Write some code in this cell to compute the number of unique word **types** and total word **tokens** in this paragraph (watch the lecture videos if you're confused about what these terms mean!). Use a whitespace tokenizer to separate words (i.e., split the string on white space using Python's split function). Be sure that the cell's output is visible in the PDF file you turn in on Gradescope.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9Fm6AQJQDFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a9f4325-4851-4661-e9d6-26472f766297"
      },
      "source": [
        "paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me\n",
        "that I was passing through the iron gates that led to the driveway.\n",
        "The drive was just a narrow track now, its stony surface covered\n",
        "with grass and weeds. Sometimes, when I thought I had lost it, it\n",
        "would appear again, beneath a fallen tree or beyond a muddy pool \n",
        "formed by the winter rains. The trees had thrown out new\n",
        "low branches which stretched across my way. I came to the house\n",
        "suddenly, and stood there with my heart beating fast and tears \n",
        "filling my eyes.'''.lower() # lowercase normalization is often useful in NLP\n",
        "\n",
        "types = 0\n",
        "tokens = 0\n",
        "\n",
        "# YOUR CODE HERE! POPULATE THE types AND tokens VARIABLES WITH THE CORRECT VALUES!\n",
        "types = len(set(paragraph.split()))  # https://docs.python.org/3.3/library/stdtypes.html?highlight=split#str.split\n",
        "tokens = len(paragraph.split())  # If sep is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings\n",
        "# DO NOT MODIFY THE BELOW LINE!\n",
        "print('Number of word types: %d, number of word tokens:%d' % (types, tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of word types: 76, number of word tokens:100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5f5YpclYjbh"
      },
      "source": [
        "##Question 2.2 (5 points)\n",
        "Now let's look at the most frequently used word **types** in this paragraph. Write some code in the below cell to print out the ten most frequently-occurring types. We have initialized a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) object that you should use for this purpose. In general, Counters are very useful for text processing in Python.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpjx2fGbh_tp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37076814-fdec-412d-dc7f-389daa422428"
      },
      "source": [
        "from collections import Counter\n",
        "c = Counter(paragraph.split())\n",
        "\n",
        "# DO NOT MODIFY THE BELOW LINES!\n",
        "for word, count in c.most_common()[:10]:\n",
        "    print(word, count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i 6\n",
            "the 6\n",
            "to 4\n",
            "a 3\n",
            "and 3\n",
            "my 3\n",
            "it 2\n",
            "that 2\n",
            "was 2\n",
            "with 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQfA98xLjIv9"
      },
      "source": [
        "##Question 2.3 (5 points)\n",
        "What do you notice about these words and their linguistic functions (i.e., parts-of-speech)? These words are known as \"stopwords\" in NLP and are often removed from the text before any computational modeling is done. Why do you think that is?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jj042SWkFvP"
      },
      "source": [
        "\n",
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "\n",
        "<blockquote>\n",
        "\n",
        "The most frequent words are usually pronouns (i, it, that), determiners (the, a, my), prepositions (to, with) and conjunctions (and). These stopwords are removed because in most cases they do not add any real value and meaning for solving the downstream task. For instance, in sentiment analysis, the pronouns and determiners are not important at all and we mostly look for positive and negative nouns and verbs. (Including these stopwords in simple models which cannot understand they are not important can cause unnecessary biases for instance towards the set of documents that include more \"the\".)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9OxWy-CYlp4"
      },
      "source": [
        "##Question 3.1 (10 points)\n",
        "In *neural* language models, we represent words with low-dimensional vectors also called *embeddings*. We use these embeddings to compute a vector representation $\\boldsymbol{x}$ of a given prefix, and then predict the probability of the next word conditioned on $\\boldsymbol{x}$. In the below cell, we use [PyTorch](https://pytorch.org), a machine learning framework, to explore this setup. We provide embeddings for the prefix \"Alice talked to\"; your job is to combine them into a single vector representation $\\boldsymbol{x}$ using [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations). \n",
        "\n",
        "*TIP: if you're finding the PyTorch coding problems difficult, you may want to run through [the 60 minutes blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)!*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su_j1JY1QG5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aa19894-eb12-480d-95cf-2ffcfe52792c"
      },
      "source": [
        "import torch\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "prefix = 'Alice talked to'\n",
        "\n",
        "# spend some time understanding this code / reading relevant documentation! \n",
        "# this is a toy problem with a 5 word vocabulary and 10-d embeddings\n",
        "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
        "vocab = {'Alice':0, 'talked':1, 'to':2, 'Bob':3, '.':4}\n",
        "\n",
        "# we need to encode our prefix as integer indices (not words) that index \n",
        "# into the embeddings matrix. the below line accomplishes this.\n",
        "# note that PyTorch inputs are always Tensor objects, so we need\n",
        "# to create a LongTensor out of our list of indices first.\n",
        "indices = torch.LongTensor([vocab[w] for w in prefix.split()])\n",
        "prefix_embs = embeddings(indices)\n",
        "print('prefix embedding tensor size: ', prefix_embs.size())\n",
        "\n",
        "# okay! we now have three embeddings corresponding to each of the three\n",
        "# words in the prefix. write some code that adds them element-wise to obtain\n",
        "# a representation of the prefix! store your answer in a variable named \"x\".\n",
        "\n",
        "### YOUR CODE HERE!\n",
        "x = torch.zeros(10)\n",
        "x = torch.sum(prefix_embs, dim=0)\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print('embedding sum: ', x)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prefix embedding tensor size:  torch.Size([3, 10])\n",
            "embedding sum:  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
            "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41LYDeoZPYI"
      },
      "source": [
        "##Question 3.2 (5 points)\n",
        "Modern language models do not use element-wise addition to combine the different word embeddings in the prefix into a single representation (a process called *composition*). What is a major issue with element-wise functions that makes them unsuitable for use as composition functions? \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MaDZekXs0C7"
      },
      "source": [
        "**Write your answer here!** Please keep it brief (i.e., 2-3 sentences).\n",
        "<blockquote>\n",
        "\n",
        "Based on modern language models and their multi-head attention mechanism for composition, we can see that 1) each word will have the similar impact in any sentence, however its impact should be determined based on the context which is the case in attention mechanism, and 2) the different aspects of one representation have equal impact, for instance, if we need to find the correct verb as the next word, we need the single/plural part of the corresponding noun and not the single/plural knowledge of other words, which can be solved by the multi-head mechanism to model various possible relations between different representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-xGz2_cZVs7"
      },
      "source": [
        "##Question 3.3 (10 points)\n",
        "One very important function in neural language models (and for basically every task we'll look at this semester) is the [softmax](https://pytorch.org/docs/master/nn.functional.html#softmax), which is defined over an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Let's say we have our prefix representation $\\boldsymbol{x}$ from before. We can use the softmax function, along with a linear projection using a matrix $W$, to go from $\\boldsymbol{x}$ to a probability distribution $p$ over the next word: $p = \\text{softmax}(W\\boldsymbol{x}). $Let's explore this in the code cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mClmHIeowL4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1894e4c3-f232-420e-c577-e2dfc7a1a73c"
      },
      "source": [
        "# remember, our goal is to produce a probability distribution over the \n",
        "# next word, conditioned on the prefix representation x. This distribution\n",
        "# is thus over the entire vocabulary (i.e., it is a 5-dimensional vector).\n",
        "# take a look at the dimensionality of x, and you'll notice that it is a \n",
        "# 10-dimensional vector. first, we need to **project** this representation\n",
        "# down to 5-d. We'll do this using the below matrix:\n",
        "\n",
        "W = torch.rand(10, 5)\n",
        "\n",
        "# use this matrix to project x to a 5-d space, and then\n",
        "# use the softmax function to convert it to a probability distribution.\n",
        "# this will involve using PyTorch to compute a matrix/vector product.\n",
        "# look through the documentation if you're confused (torch.nn.functional.softmax)\n",
        "# please store your final probability distribution in the \"probs\" variable.\n",
        "\n",
        "### YOUR CODE HERE\n",
        "probs = torch.rand(5)\n",
        "probs = torch.matmul(x, W)\n",
        "probs = torch.softmax(probs, dim=0)\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE!\n",
        "print('probability distribution', probs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability distribution tensor([0.0718, 0.0998, 0.1331, 0.6762, 0.0191], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOlrqnSqZ3H8"
      },
      "source": [
        "##Question 3.4 (15 points)\n",
        "So far, we have looked at just a single prefix (\"Alice talked to\"). In practice, it is common for us to compute many prefixes in one computation, as this enables us to take advantage of GPU parallelism and also obtain better gradient approximations (we'll talk more about the latter point later). This is called *batching*, where each prefix is an example in a larger batch. Here, you'll redo the computations from the previous cells, but instead of having one prefix, you'll have a batch of two prefixes. The final output of this cell should be a 2x5 matrix that contains two probability distributions, one for each prefix. **NOTE: YOU WILL LOSE POINTS IF YOU USE ANY LOOPS IN YOUR ANSWER!** Your code should be completely vectorized (a few large computations is faster than many smaller ones)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZarWwkESM7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5eb950a-1b5c-40f0-962b-e1d4fc988046"
      },
      "source": [
        "\n",
        "# for this problem, we'll just copy our old prefix over three times\n",
        "# to form a batch. in practice, each example in the batch would be different.\n",
        "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
        "batch_embs = embeddings(batch_indices)\n",
        "print('batch embedding tensor size: ', batch_embs.size())\n",
        "\n",
        "# now, follow the same procedure as before:\n",
        "# step 1: compose each example's embeddings into a single representation\n",
        "# using element-wise addition. HINT: check out the \"dim\" argument of the torch.sum function!\n",
        "x = torch.sum(batch_embs, dim=1)\n",
        "\n",
        "# step 2: project each composed representation into a 5-d space using matrix W\n",
        "# step 3: use the softmax function to obtain a 2x5 matrix with the probability distributions\n",
        "\n",
        "# please store this probability matrix in the \"batch_probs\" variable.\n",
        "\n",
        "batch_probs = torch.rand(2,5)\n",
        "batch_probs = torch.matmul(x, W)\n",
        "batch_probs = torch.softmax(batch_probs, dim=1)\n",
        "\n",
        "### DO NOT MODIFY THE BELOW LINE\n",
        "print(\"batch probability distributions:\", batch_probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch embedding tensor size:  torch.Size([2, 3, 10])\n",
            "batch probability distributions: tensor([[0.0718, 0.0998, 0.1331, 0.6762, 0.0191],\n",
            "        [0.0718, 0.0998, 0.1331, 0.6762, 0.0191]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTxHxdG5BpQU"
      },
      "source": [
        "## Question 4 (20 points)\n",
        "\n",
        "Choose  one  paper  from  [EMNLP 2021](https://aclanthology.org/events/emnlp-2021/#2021-emnlp-main) that you find interesting. A good way to do this is by scanning the titles and abstracts; there are hundreds of papers so take your time before selecting one!  Then, write a summary in  your own words of the paper you chose. Your summary should answer the following questions: what is its motivation? Why should anyone care about it? Were there things in the paper that you didn't understand at all? What were they? Fill out the below cell, and make sure to write 2-4 paragraphs for the summary to receive full credit! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpg8eU9wNBci"
      },
      "source": [
        "**Title of paper**: Visually Grounded Reasoning across Languages and Cultures\n",
        "\n",
        "**Authors**: Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, Desmond Elliott\n",
        "\n",
        "**Conference name**: EMNLP 2021\n",
        "\n",
        "**URL**: https://aclanthology.org/2021.emnlp-main.818/\n",
        "\n",
        "**Your summary**:\n",
        "<blockquote>\n",
        "\n",
        "* what is its motivation? \\\\\n",
        "The motivations of this paper are the limitations and issues with the Imagenet dataset. Imagenet is created based on concepts extracted from Wordnet lexical database and is not general nor representative of different langauges and cultures. Imagenet concepts are not universal either, like having different types of dogs. Therefore, they create a new annotation protocol driven by native speakers.\n",
        "* Why should anyone care about it? \\\\\n",
        "Imagenet is a very popular and integral part of computer vision and recently vision Transformers. Consequently, it is cruical to consider its shortcomings and disadvantages. Specifcally, not being universal and representative of different cultures makes it very stricting dataset with less ability to generalize. So, this paper, which introduces a new dataset based on images and captions by native speakers, can be a great data and also show the correct path to follow for making our models and datasets more inclusive. They have followed the best process they could do to prevent biases in the dataset, from selecting the universal categories and then letting different cultures add the subgroups that are more important to their own culture and even asking annotators to choose images that best captures their own culture such as the different images for basketball in the paper. Moreover, this dataset brings up a challenging task, grounded language-vision reasoning, which can help us as a becnhmark for making more general, robust, and less biased models. (It has shown in the paper that we have issues in this part)\n",
        "* Were there things in the paper that you didn't understand at all? What were they? \\\\\n",
        "I just did not understand the small details of model's architecture for this task (not the inputs and outputs, but how different modality models are fused together) such as UNITER, albeit it was a minor point, mentioned in the appendix.\n",
        "\n",
        "My highlighted version of paper: https://drive.google.com/file/d/1H3ELDeD4RwZWu0TqCtvmQbeYbjBevXn2/view?pli=1\n",
        "\n",
        "Also check out https://aclanthology.org/2021.blackboxnlp-1.29/  :))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgpxuzKuZ46U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}